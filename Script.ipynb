{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cacf5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2365\tSaluki\tBasenji\tNorwich terrier\tShetland sheepdog\n",
    "#2365\t29(Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4054d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import glob\n",
    "from PIL import Image \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile,join\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b98804b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths and variables\n",
    "import glob\n",
    "dog_images = glob.glob('C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/GowthamImages/*/*')\n",
    "annotations_dir = 'C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/GowthamAnnotations'\n",
    "annotations = glob.glob('C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/GowthamAnnotations/*/*')\n",
    "cropped = \"./Cropped/\"\n",
    "img_size = 299  # For Xception input\n",
    "train_dir = './Cropped'  # './Images'\n",
    "batch_size_training = 256\n",
    "batch_size_validation = 256\n",
    "input_shape = (img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01754de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function to extract bounding boxes\n",
    "def get_bounding_boxes(annot):\n",
    "    xml = annot\n",
    "    tree = ET.parse(xml)\n",
    "    root = tree.getroot()\n",
    "    objects = root.findall('object')\n",
    "    bbox = []\n",
    "    for o in objects:\n",
    "        bndbox = o.find('bndbox')\n",
    "        xmin = int(bndbox.find('xmin').text)\n",
    "        ymin = int(bndbox.find('ymin').text)\n",
    "        xmax = int(bndbox.find('xmax').text)\n",
    "        ymax = int(bndbox.find('ymax').text)\n",
    "        bbox.append((xmin, ymin, xmax, ymax))\n",
    "    return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "862e7e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output directory for cropped and resized images\n",
    "cropped = \"./Cropped/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eb79f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations=glob.glob('/C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/GowthamAnnotations/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395e6d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Get image path from annotation path ########\n",
    "def get_image(annot):\n",
    "    img_path = '/C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/GowthamImages/'\n",
    "    file = annot.split('/')\n",
    "    img_filename = img_path + file[-2]+'/'+file[-1]+'.jpg'\n",
    "    return img_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1cb4ff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dog_images)):\n\u001b[1;32m----> 2\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m get_bounding_boxes(annotations[i])\n\u001b[0;32m      3\u001b[0m     dog \u001b[38;5;241m=\u001b[39m get_image(annotations[i])\n\u001b[0;32m      4\u001b[0m     im \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(dog)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(dog_images)):\n",
    "    bbox = get_bounding_boxes(annotations[i])\n",
    "    dog = get_image(annotations[i])\n",
    "    im = Image.open(dog)\n",
    "    for j in range(len(bbox)):\n",
    "        im2 = im.crop(bbox[j])\n",
    "        im2 = im2.resize((128,128), Image.ANTIALIAS)\n",
    "        new_path = dog.replace('/C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/')\n",
    "        new_path = new_path.replace('.jpg','-' + str(j) + '.jpg')\n",
    "        im2=im2.convert('RGB')\n",
    "        head, tail = os.path.split(new_path)\n",
    "        Path(head).mkdir(parents=True, exist_ok=True)\n",
    "        im2.save(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a60ccf",
   "metadata": {},
   "source": [
    "###### Converting Color images to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "419aa010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io, color\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Load image and convert it to grayscale\n",
    "def load_and_convert_to_grayscale(image_path):\n",
    "    image = io.imread(image_path)\n",
    "    grayscale_image = color.rgb2gray(image)\n",
    "    return grayscale_image\n",
    "\n",
    "# Paths to images in each class\n",
    "class_images = {\n",
    "    \"Saluki\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02091831-Saluki/n02091831_1400-0.jpg\",\n",
    "    \"Norwich Terrier\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02094258-Norwich_terrier/n02094258_847-0.jpg\",\n",
    "    \"Shetland Sheepdog\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02105855-Shetland_sheepdog/n02105855_5719-0.jpg\",\n",
    "    \"Basenji\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02110806-basenji/n02110806_4395-0.jpg\"\n",
    "}\n",
    "\n",
    "\n",
    "# Convert images\n",
    "grayscale_images = {class_name: load_and_convert_to_grayscale(path) for class_name, path in class_images.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758e275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "import numpy as np\n",
    "\n",
    "# Define angle calculation function\n",
    "def angle(dx, dy):\n",
    "    \"\"\"Calculate the angles between horizontal and vertical operators.\"\"\"\n",
    "    return np.mod(np.arctan2(dy, dx), np.pi)\n",
    "\n",
    "# Apply Sobel filter to calculate edge angles\n",
    "def compute_edge_angles(image):\n",
    "    sobel_h = filters.sobel_h(image)  # Sobel horizontal\n",
    "    sobel_v = filters.sobel_v(image)  # Sobel vertical\n",
    "    angle_sobel = angle(sobel_h, sobel_v)\n",
    "    return angle_sobel\n",
    "\n",
    "# Calculate edge angles for each grayscale image\n",
    "edge_angles = {class_name: compute_edge_angles(image) for class_name, image in grayscale_images.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63235a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Display grayscale image and edge angles\n",
    "def plot_image_and_angles(class_name, grayscale_image, edge_angle_image):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(grayscale_image, cmap='gray')\n",
    "    axes[0].set_title(f\"{class_name} - Grayscale\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(edge_angle_image, cmap='hsv')  # hsv colormap for better angle visualization\n",
    "    axes[1].set_title(f\"{class_name} - Edge Angles\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Plot images and angles for each class\n",
    "for class_name in grayscale_images:\n",
    "    plot_image_and_angles(class_name, grayscale_images[class_name], edge_angles[class_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311fba3",
   "metadata": {},
   "source": [
    "###### Edge Histograms(36 Bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "005413a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters, exposure\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Compute the histogram with 36 bins for edge angles\n",
    "def compute_histogram(edge_angle_image, bins=36):\n",
    "    # Compute the histogram of the edge angles using skimage.exposure.histogram\n",
    "    hist, _ = exposure.histogram(edge_angle_image, nbins=bins)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e903171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grayscale image and its corresponding edge angle histogram side by side\n",
    "def plot_image_and_histogram(image, edge_angle_image, hist, bins=36):\n",
    "    # Create a figure with two subplots (1 row, 2 columns)\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot the grayscale image\n",
    "    ax[0].imshow(image, cmap='gray')\n",
    "    ax[0].set_title('Grayscale Image')\n",
    "    ax[0].axis('off')  # Hide axis labels for the image\n",
    "    \n",
    "    # Plot the histogram with bin numbers on the x-axis\n",
    "    ax[1].bar(range(1, bins+1), hist, align='center')\n",
    "    ax[1].set_title('Edge Angle Histogram with 36 bins')\n",
    "    ax[1].set_xlabel('Bins')\n",
    "    ax[1].set_ylabel('Pixel Count')\n",
    "    ax[1].set_xticks(range(1, bins+1, 3))  # Set x-ticks from 1 to 36, spaced by 3\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout so everything fits nicely\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbcc862-eb7c-43e2-acf2-2e89ce046471",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name, grayscale_image in grayscale_images.items():\n",
    "    # Compute edge angles for the grayscale image\n",
    "    print(f\"Processing {class_name} image...\")\n",
    "    edge_angle_image = compute_edge_angles(grayscale_image)\n",
    "    \n",
    "    # Compute the histogram of edge angles\n",
    "    hist = compute_histogram(edge_angle_image)\n",
    "    \n",
    "    # Plot the grayscale image and corresponding histogram side by side\n",
    "    plot_image_and_histogram(grayscale_image, edge_angle_image, hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cf38c",
   "metadata": {},
   "source": [
    "###### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9350bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Function to compare two histograms using different distance metrics\n",
    "def compare_histograms(hist1, hist2):\n",
    "    # Reshape the histograms to be 2D arrays (required by pairwise_distances)\n",
    "    hist1 = np.array(hist1).reshape(1, -1)\n",
    "    hist2 = np.array(hist2).reshape(1, -1)\n",
    "    \n",
    "    # Compute Euclidean, Manhattan, and Cosine distances\n",
    "    euclidean_dist = pairwise_distances(hist1, hist2, metric='euclidean')[0][0]\n",
    "    manhattan_dist = pairwise_distances(hist1, hist2, metric='manhattan')[0][0]\n",
    "    cosine_dist = pairwise_distances(hist1, hist2, metric='cosine')[0][0]\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Euclidean Distance: {euclidean_dist}\")\n",
    "    print(f\"Manhattan Distance: {manhattan_dist}\")\n",
    "    print(f\"Cosine Distance: {cosine_dist}\")\n",
    "\n",
    "# Example usage: Pick two histograms from your constructed histograms\n",
    "# For example, let's compare the histograms for \"Saluki\" and \"Norwich Terrier\"\n",
    "hist1 = compute_histogram(compute_edge_angles(grayscale_images['Saluki']))\n",
    "hist2 = compute_histogram(compute_edge_angles(grayscale_images['Norwich Terrier']))\n",
    "\n",
    "# Compare the histograms\n",
    "compare_histograms(hist1, hist2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6e363",
   "metadata": {},
   "source": [
    "###### HOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07acbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import io, color\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import exposure\n",
    "\n",
    "# Pick one image (e.g., \"Saluki\")\n",
    "image = grayscale_images[\"Saluki\"]\n",
    "\n",
    "# Compute HOG descriptors and the HOG image for visualization\n",
    "hog_descriptors, hog_image = hog(image, orientations=9, pixels_per_cell=(8, 8),\n",
    "                                 cells_per_block=(2, 2), block_norm='L2-Hys',\n",
    "                                 visualize=True, feature_vector=True)\n",
    "\n",
    "# Visualize the original image and the HOG image side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Original grayscale image\n",
    "ax1.imshow(image, cmap='gray')\n",
    "ax1.set_title('Original Grayscale Image')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Rescale HOG image for better visualization\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "# HOG image\n",
    "ax2.imshow(hog_image_rescaled, cmap='gray')\n",
    "ax2.set_title('HOG Descriptor Visualization')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0666ee0c",
   "metadata": {},
   "source": [
    "###### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d7adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import filters, exposure\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def compute_edge_histogram(img):\n",
    "    \"\"\"Compute the edge histogram of the image.\"\"\"\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    angle_sobel = np.mod(np.arctan2(filters.sobel_v(gray_img), filters.sobel_h(gray_img)), np.pi)\n",
    "    hist, _ = exposure.histogram(angle_sobel.flatten(), nbins=36)\n",
    "    return hist\n",
    "\n",
    "def process_images(folder_path):\n",
    "    \"\"\"Process all images in a folder and return their edge histograms.\"\"\"\n",
    "    histograms = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        if image_path.lower().endswith(('.jpg', '.png')):  # Handle both jpg and png formats\n",
    "            img = cv2.imread(image_path)\n",
    "            hist = compute_edge_histogram(img)\n",
    "            histograms.append(hist)\n",
    "    return histograms\n",
    "\n",
    "def plot_2d_points(reduced_data, num_classes):\n",
    "    \"\"\"Plot 2D points after PCA with different colors for different classes.\"\"\"\n",
    "    colors = ['blue', 'red', 'green', 'orange']  # Different colors for the four classes\n",
    "    points_per_class = reduced_data.shape[0] // num_classes  # Calculate how many points per class\n",
    "    for i in range(num_classes):\n",
    "        start_index = i * points_per_class\n",
    "        end_index = (i + 1) * points_per_class\n",
    "        plt.scatter(reduced_data[start_index:end_index, 0], \n",
    "                    reduced_data[start_index:end_index, 1], \n",
    "                    color=colors[i], label=f'Class {i + 1}')\n",
    "    \n",
    "    plt.title('2D Points after PCA and Standard Scaling')\n",
    "    plt.xlabel('Principal Component 1 (Standardized)')\n",
    "    plt.ylabel('Principal Component 2 (Standardized)')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Folder paths for images from four classes\n",
    "class_dirs = {\n",
    "    \"Saluki\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02091831-Saluki\",\n",
    "    \"Norwich Terrier\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02094258-Norwich_terrier\",\n",
    "    \"Shetland Sheepdog\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02105855-Shetland_sheepdog\",\n",
    "    \"Basenji\": \"C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/Cropped/n02110806-basenji\"\n",
    "}\n",
    "\n",
    "# Initialize a list to store all histograms\n",
    "all_histograms = []\n",
    "\n",
    "# Process images and compute edge histograms for each class\n",
    "for class_dir in class_dirs.values():\n",
    "    class_histograms = process_images(class_dir)\n",
    "    all_histograms.extend(class_histograms)  # Add histograms to the list\n",
    "\n",
    "# Convert histograms to a NumPy array\n",
    "all_histograms = np.array(all_histograms)\n",
    "\n",
    "# Perform PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Use StandardScaler to normalize the data before applying PCA\n",
    "scaler = StandardScaler()\n",
    "reduced_data = pca.fit_transform(scaler.fit_transform(all_histograms))\n",
    "\n",
    "# Plot the 2D points with different colors for each class without using class labels\n",
    "plot_2d_points(reduced_data, num_classes=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e29223e",
   "metadata": {},
   "source": [
    "###### Not Separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38321a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f650723-c38f-44f1-8feb-569a841fac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21024648-e512-4cad-9f14-bf1dbebfa090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using pandas to load the JSON file\n",
    "train_df = pd.read_json('C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/train.json', lines=True)\n",
    "test_df = pd.read_json('C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/test.json', lines=True)\n",
    "val_df = pd.read_json('C:/Users/Gowtham reddy/Downloads/Gowtham_DM_Assignment1/Gowtham_DM_Assignment1/validation.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30200dc7-c5e7-4930-838f-0dc97fdecf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41c6c8-b527-412a-9c86-3dfc27085209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc91519-1e4a-4205-ac1d-c3df37fbde2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, and hashtags\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|@\\S+|#\\S+', '', tweet)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize the tweet\n",
    "    words = word_tokenize(tweet)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join the words back into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'text' column in each dataset\n",
    "train_df['processed_text'] = train_df['Tweet'].apply(preprocess_tweet)\n",
    "test_df['processed_text'] = test_df['Tweet'].apply(preprocess_tweet)\n",
    "val_df['processed_text'] = val_df['Tweet'].apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ced35-5d90-4dd0-a339-2d5ab9308d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "X_train = train_df['processed_text']\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Get the dimensionality of the count vector representation\n",
    "count_dim = X_train_counts.shape\n",
    "print(f'Count Vectorizer Dimensionality: {count_dim}')\n",
    "\n",
    "# Get the dimensionality of the TF-IDF vector representation\n",
    "tfidf_dim = X_train_tfidf.shape\n",
    "print(f'TF-IDF Vectorizer Dimensionality: {tfidf_dim}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3667e893-a822-45de-b081-aaeb1be8b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the emotion columns\n",
    "emotion_columns = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', \n",
    "                   'optimism', 'pessimism', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "# Function to get the emotion label\n",
    "def get_label(row):\n",
    "    for emotion in emotion_columns:\n",
    "        if row[emotion] == True:\n",
    "            return emotion\n",
    "    return None  # In case no emotion is labeled True\n",
    "\n",
    "# Create the 'label' column\n",
    "train_df['label'] = train_df.apply(get_label, axis=1)\n",
    "test_df['label'] = test_df.apply(get_label, axis=1)\n",
    "val_df['label'] = val_df.apply(get_label, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fb8272-a2c0-43cc-a06d-9b4a58f73ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the four classes\n",
    "selected_classes = ['anger', 'anticipation', 'disgust', 'fear']\n",
    "\n",
    "# Filter the training data for only the selected classes\n",
    "filtered_train_df = train_df[train_df['label'].isin(selected_classes)]\n",
    "\n",
    "# Prepare feature representations for token counts and tf-idf\n",
    "X_filtered_count = count_vectorizer.transform(filtered_train_df['processed_text'])\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(filtered_train_df['processed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd42a1-5dca-4035-a3a2-de3f978976b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Get the corresponding labels\n",
    "y_filtered = filtered_train_df['label']\n",
    "\n",
    "# Perform PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_count_pca = pca.fit_transform(X_filtered_count.toarray())\n",
    "X_tfidf_pca = pca.fit_transform(X_filtered_tfidf.toarray())\n",
    "\n",
    "# Assign colors for each class\n",
    "colors = {\n",
    "    'anger': 'blue',\n",
    "    'anticipation': 'red',\n",
    "    'disgust': 'green',\n",
    "    'fear': 'purple'\n",
    "}\n",
    "\n",
    "# Plot function for 2D points\n",
    "def plot_2d(X, y, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for label in selected_classes:\n",
    "        indices = np.where(y == label)\n",
    "        plt.scatter(X[indices, 0], X[indices, 1], c=colors[label], label=label, alpha=0.6)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('PCA Component 1')\n",
    "    plt.ylabel('PCA Component 2')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot for token count features\n",
    "plot_2d(X_count_pca, y_filtered, 'PCA of Token Count Features')\n",
    "\n",
    "# Plot for tf-idf features\n",
    "plot_2d(X_tfidf_pca, y_filtered, 'PCA of TF-IDF Features')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
